---
title: "LumA vs LumB - SVM"
author: "Poon Chi Lam"
date: "6/20/2019"
output: html_document
---

- Datasets: TCGA BRCA RNA-seq and MetaBric microarray
- Filtering: 50% low-mean genes out
- Feature type: expression values
- Feature selection: differential expression analysis using limma
- Normalization: quantile normalization
- Algorithm: support vector machine

- Feature sets:

1. All DE genes (~5k genes) (adj.P.Val < 0.01)
2. Super DE genes (adj.P.Val < 0.01 & abs(logFC) > 1.5)
3. Top 50 DEGs
4. Top 100 DEGs
5. Top 500 DEGs
6. Top 1000 DEGs
7. Top 2000 DEGs

One as training data and the other as testing, that is doing twice.

```{R set up}
suppressPackageStartupMessages({
  library(RColorBrewer)
  library(tidyr)
  library(dplyr)
  library(reshape2)
  library(ggplot2)
  library(caret)
  library(pROC)
  library(caTools)
  library(kernlab)
})
```


## Functions
```{R}
# 5-fold cross validation
svm.CV <- function(data.train, lb.train) {
  results.list <- list()
  data.train <- cbind.data.frame(data.train, type=lb.train)
  folds <- createFolds(lb.train, k=5)
  
  for (i in 1:5) {
    results.list[[as.character(i)]] <- list()
    
    # 1/5 for validation, 4/5 for training
    cv.train <- data.train[-folds[[i]], ]
    cv.train.labels <- lb.train[-folds[[i]]]
    cv.valid <- data.train[folds[[i]], ]
    cv.valid.labels <- lb.train[folds[[i]]]
    
    # Build the model
    ksvm.model <- ksvm(type~., cv.train, kernel = 'rbfdot', type="C-svc", kpar = 'automatic', prob.model=TRUE)
    predictionLabels <- predict(ksvm.model, newdata=cv.valid, type='response')
    predictionScores <- predict(ksvm.model, newdata=cv.valid, type='prob')
    
    # Save performances
    cm <- confusionMatrix(data=as.factor(predictionLabels), reference=as.factor(cv.valid.labels))
    sens <- as.numeric(cm$byClass['Sensitivity'])
    spec <- as.numeric(cm$byClass['Specificity'])
    youden <- sens + spec - 1
    names(youden) <- gsub('Class: ', '', rownames(cm$byClass))
    
    results.list[[i]][['trueLabels']] <- cv.valid.labels
    results.list[[i]][['predictionLabels']] <- predictionLabels
    results.list[[i]][['predictionScores']] <- predictionScores
    results.list[[i]][['ConfusionMatrix']] <- cm
    results.list[[i]][['ACC']] <- as.numeric(cm$overall['Accuracy'])
    results.list[[i]][['Youden']] <- youden
    results.list[[i]][['AUC']] <- auc(roc(response=as.vector(cv.valid.labels), predictor=as.vector(predictionScores[ ,1])))
    results.list[[i]][['model']] <- ksvm.model
    #print(paste0("Fold ", i, " AUC:", round(results.list[[i]][['AUC']], 5)))
  }
  results.list
}


# Get CV results
getOverall <- function(results.list) {
  overall.results <- list()
  
  all.true.labels <- c()
  all.predict.labels <- c()
  all.predict.scores <- c()
  for (f in 1:length(results.list)) {
    all.true.labels <- c(all.true.labels, as.character(results.list[[f]][['trueLabels']]))
    all.predict.labels <- c(all.predict.labels, as.character(results.list[[f]][['predictionLabels']]))
    all.predict.scores <- rbind(all.predict.scores, results.list[[f]][['predictionScores']])
  }
  
  all.cm <- confusionMatrix(data=as.factor(all.predict.labels), reference=as.factor(all.true.labels))
  overall.results[['ACC']] <- as.numeric(all.cm$overall['Accuracy'])
  overall.results[['Sensitivity']] <- as.numeric(all.cm$byClass['Sensitivity']) -> all.sens
  overall.results[['Specificity']] <- as.numeric(all.cm$byClass['Specificity']) -> all.spec
  all.youden <- all.sens + all.spec - 1
  names(all.youden) <- gsub('Class: ', '', rownames(all.cm$byClass))
  overall.results[['Youden']] <- all.youden
  overall.results[['predictionScores']] <- all.predict.scores
  overall.results[['cm']] <- all.cm
  overall.results[['AUC']] <- auc(roc(response=as.vector(all.true.labels), predictor=as.vector(all.predict.scores[,1])))
  overall.results[['trueLabels']] <- all.true.labels
  overall.results
}


# Train a model using whole training data
trainModel <- function(data.train, labels.train) {
  data.train <- cbind.data.frame(data.train, type=labels.train)
  full.model <- ksvm.model <- ksvm(type~., data.train, kernel = 'rbfdot', type="C-svc", kpar = 'automatic', prob.model=TRUE)
  full.model
}


# Test on testing data
testing <- function(data.test, labels.test, model) {
  test.results <- list()
  data.test <- cbind.data.frame(data.test, type=labels.test)
  
  predictionLabels.test <- predict(model, newdata = data.test, type = 'response')
  predictionLabels.test <- factor(predictionLabels.test, levels = levels(labels.test))
  predictionScores.test <- predict(model, newdata = data.test, type = 'prob')
  
  cm <- confusionMatrix(data=predictionLabels.test, reference=labels.test)
  sensitivity <- as.numeric(cm$byClass['Sensitivity'])
  specificity <- as.numeric(cm$byClass['Specificity'])
  youden <- sensitivity + specificity - 1
  names(youden) <- gsub('Class: ', '', rownames(cm$byClass))
  test.acc <- as.numeric(cm$overall['Accuracy'])
  
  test.results[['ACC']] <- test.acc
  test.results[['predictionLabels']] <- predictionLabels.test
  test.results[['predictionScores']] <- predictionScores.test
  test.results[['cm']] <- cm
  test.results[['AUC']] <- auc(roc(response=as.vector(labels.test), predictor=as.vector(predictionScores.test[,1])))
  test.results[['Youden']] <- youden
  print(paste0("AUC:", round(test.results$AUC, 5)))
  test.results
}


# Get all results
doSVM <- function(ori.train, ori.test, lb.train, lb.test, featureSet) {
  results <- list()
  s.train <- ori.train[featureSet,]
  s.test <- ori.test[featureSet,]
  
  results[['cv']] <- svm.CV(as.data.frame(t(s.train)), lb.train)
  
  results[['cv.overall']] <- getOverall(results[['cv']])
  print(results[['cv.overall']]$ACC); print(results[['cv.overall']]$cm); print(results[['cv.overall']]$Youden); print(results[['cv.overall']]$AUC)
  
  results[['full.model']] <- trainModel(as.data.frame(t(s.train)), lb.train)
  results[['test']] <- testing(as.data.frame(t(s.test)), lb.test, results[['full.model']])
  print(results[['test']]$AUC); print(results[['test']]$ACC); print(results[['test']]$cm); print(results[['test']]$Youden)
  results
}
```


## TCGA as training
```{R}
data.dir <- '~/GoogleDrive/pjs/kTSP/data'
load(file.path(data.dir, "tcga.DEG.Rdata"))

### 1. All DEGs
tcga.all <- doSVM(nm.data.train, nm.data.test, lb.train, lb.test, rownames(nm.data.train))

### 2. Super DEGs
sDEG <- rownames(tt[(tt$adj.P.Val < 0.01) & (abs(tt$logFC) > log2(1.5)), ]) 
tcga.super <- doSVM(nm.data.train, nm.data.test, lb.train, lb.test, sDEG)

### The rest of the feature sets
for (s in c(50, 100, 500, 1000, 2000)) {
  print(paste0('Top ', s, ' DEGs...'))
  set <- rownames(tt[1:s,])
  assign(paste0('tcga.', s), doSVM(nm.data.train, nm.data.test, lb.train, lb.test, set))
}

save(tcga.all, tcga,super, tcga.50, tcga.100, tcga.500, tcga.1000, tcga.2000, file=file.path(data.dir, 'tcga.SVM.results.Rdata'))
```


## MetaBric as training
```{R}
data.dir <- '~/GoogleDrive/pjs/kTSP/data'
load(file.path(data.dir, "mb.DEG.Rdata"))

### 1. All DEGs
mb.all <- doSVM(nm.data.train, nm.data.test, lb.train, lb.test, rownames(nm.data.train))

### 2. Super DEGs
sDEG <- rownames(tt[(tt$adj.P.Val < 0.01) & (abs(tt$logFC) > log2(1.5)), ]) 
mb.super <- doSVM(nm.data.train, nm.data.test, lb.train, lb.test, sDEG)

### The rest of the feature sets
for (s in c(50, 100, 500, 1000, 2000)) {
  print(paste0('Top ', s, ' DEGs...'))
  set <- rownames(tt[1:s,])
  assign(paste0('mb.', s), doSVM(nm.data.train, nm.data.test, lb.train, lb.test, set))
}

save(mb.all, mb.super, mb.50, mb.100, mb.500, mb.1000, mb.2000, file=file.path(data.dir, 'mb.SVM.results.Rdata'))
```